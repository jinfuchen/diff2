\section{Related Work} \label{sec:relatedwork}
We discuss related work along with two directions: prior work that analyzes performance test results and that leverages logs to detect performance-related anomalies.

\subsection{Analyzing performance test results}
Prior research has proposed many automated techniques to analyze the results of performance tests~\citep{DBLP:conf/icst/GaoJBL16,DBLP:journals/tse/JiangH15}.
Initially, the Queuing Network Model (QNM)~\citep{DBLP:books/daglib/0076254} has been proposed to model the performance of a software system based on the queuing theory. 
Based on QNM, ~\citet{DBLP:conf/icac/BarnaLG11} proposes an autonomic performance testing framework to locate the software and hardware bottlenecks in the system. They use a two-layer QNM to automatically target hardware and software resources utilization limits (e.g., hardware utilization, web container threads number, and response time). 

Due to the increasing complexity of software systems and their behaviors, the QNM-based performance model becomes insufficient.
Therefore, prior work uses statistical methods to assist in performance analysis.
~\citet{DBLP:conf/sosp/CohenZGSKF05} present an approach that captures the signatures of the states of a running system and then cluster such signatures to detect recurrent or similar performance problems. 
~\citet{DBLP:conf/apsec/NguyenAJHNF11,DBLP:conf/wosp/NguyenAJHNF12} use control charts to analyze performance metrics across test runs to detect performance regressions.
~\citet{DBLP:conf/csmr/MalikJAHFH10,DBLP:conf/icse/MalikHH13} use principle component analysis (PCA) to reduce the large number of performance metrics to a smaller set, in order to reduce the efforts of performance analysts.

Prior work also uses machine learning techniques for performance analysis~\citep{DBLP:conf/wosp/ShangHNF15,DBLP:conf/wosp/XiongPZG13,Foo:2015:ICS:2819009.2819034,DBLP:conf/icdm/LimLZFTLDZ14,DBLP:conf/wosp/DidonaQRT15}.
~\citet{DBLP:conf/wosp/ShangHNF15} proposes an approach that automatically selects target performance metrics from a larger set, in order to model system performance. 
~\citet{DBLP:conf/wosp/XiongPZG13} propose an approach that automatically identifies the system metrics that are highly related to system performance and detects changes in the system metrics that lead to changes in the system performance. 
~\citet{Foo:2015:ICS:2819009.2819034} build ensembles of models and association rules to detect performance regressions in heterogeneous environments, in order to achieve high precision and recall in the detection results.

Different from existing approaches that analyze the results of in-house performance tests, our approach builds black-box performance models that leverage performance metrics and readily available logs to detect performance regressions in the field.

\subsection{Leveraging logs to detect performance-related anomalies}

Prior research proposes various approaches that leverage execution logs to detect performance-related anomalies~\citep{DBLP:journals/tse/JiangH15,DBLP:conf/sosp/XuHFPJ09,DBLP:conf/icdcs/TanKGN10,DBLP:conf/sigsoft/HeLLZLZ18}. 
~\citet{DBLP:conf/icsm/JiangHHF09} propose a framework that automatically generates a report to detect and rank potential performance problems and the associated log sequences. 
~\citet{DBLP:conf/sosp/XuHFPJ09} extract event features from the execution logs and leverage PCA to detect performance-related anomalies. 
~\citet{DBLP:conf/icdcs/TanKGN10} present a state-machine view from the execution logs of Hadoop to understand the system behavior and debug performance problems. 
~\citet{DBLP:journals/ase/SyerSJH17} propose to leverage execution logs to continuously validate whether the workloads in the performance tests are reflective of the field workloads. 
~\citet{DBLP:conf/icsm/SyerJNHNF13} also propose to combine execution logs and performance metrics to diagnose memory-related performance issues. 
Similarly,~\citet{DBLP:conf/sigsoft/HeLLZLZ18} correlates the clusters of log sequences with system performance metrics to identify impactful system problems (e.g., request latency and service availability).

In comparison, our approach leverages the relationship between the logs and performance metrics from the field to detect performance regressions between two versions of a software system.

