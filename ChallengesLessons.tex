\section{Challenges and Lessons Learned} \label{sec:challenges}
In this section, we discuss our faced challenges and learned lessons during applying our approach in the production environment of an industrial setting where a large amount of customers worldwide access the system on a daily basis.

\newcounter{ChallengeCount}
\setcounter{ChallengeCount}{0}

%\subsection{Detection time and overhead is crucial for the successful application of our approach in practice.}

%\subsubsection{Determine performance metrics sampling frequency:}
%\subsubsection{\textbf{Challenge: determining the sampling frequency of performance metrics}}
\stepcounter{ChallengeCount}
\subsection*{C\arabic{ChallengeCount}: Determining the sampling frequency of performance metrics}
\noindent\textbf{Challenge.}
Our approach uses both logs and performance metrics as the input data to our black-box models. We use the logs that are automatically generated by the web servers, such as the Jetty, Tomcat, and IIS (Internet Information Services) web servers. 
The performance metrics (e.g., CPU, I/O) of the systems are collected using tools (e.g., \emph{Pidstat}).
%to collect the performance counter of the web server or database server, . 
A higher sampling frequency of the performance metrics can capture the system performance more accurately. However, a higher sampling frequency would also introduce more performance overhead. Since we need to apply our approach in the production environment, it is necessary to produce as low performance overhead as possible.

\noindent\textbf{Solution.}
At a first attempt, we intuitively chose 10 seconds as the sampling interval of the performance metrics. After we deployed our approach in production, we found that there is 0.5\%-0.8\% CPU overhead each time when our approach is collecting the performance metrics, and the overhead happens six times in 1 minute. Such a monitoring overhead cannot be ignored, especially when the system is serving heavy workloads. After working closely with the IT staffs from our industrial partner, we gained a deeper understanding of how the sampling frequency of performance metrics impact the monitoring overhead. Finally, we agreed that collecting the performance metrics for every 30 seconds would be a good balance between reducing the monitoring overhead and ensuring an accuracy measurement of the system performance.

\noindent\textbf{Lessons learned.}
Monitoring a system usually comes with the monitoring overhead.
A higher monitoring frequency can provide a better monitoring accuracy at the cost of a larger monitoring overhead, which is usually undesirable when the system is serving large workloads.
%The higher performance metrics sampling frequency provide the collected performance counters better more accurate reflection of the system performance, however, it will also introduce more performance overhead on the production server, especially when server is busy. 
Finding a good balance between the monitoring accuracy and the overhead is crucial for successful adoptions of similar approaches in practice.


\stepcounter{ChallengeCount}
\subsection*{C\arabic{ChallengeCount}: Reducing the time cost of performance regression detection}
\noindent\textbf{Challenge.}
We choose random forest as our final black-box model to detect performance regressions, as random forest achieves the best results for modeling the performance of the studied systems (see RQ1).
%Random forest is an ensemble learning method for classification or regression that operates by a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees.  
A random forest model contains a configurable multitude of decisions trees and takes the average output of the individual decision trees as the final output.
%One needs to configure the number of trees when training a random forest classifier. 
%A larger number of trees usually result in a more accurate and robust model, at the cost of longer training and prediction time.
%More trees mean better accuracy of the model, but it also makes the model more complex and takes more time on both training and testing. 
%Our approach would build multiple models based on the size of testing data. 
At first, we started with the default number of trees (i.e., 500 trees)~\citep{R-RandomForest} and spent 5-6 hours to detect performance regressions for the industry system. 
The industrial practitioners usually have an early need of checking if there is performance regression between the current version and multiple historical versions, which makes our approaches difficult to be adopted in a fast-paced development environment (e.g., an agile environment).

\noindent\textbf{Solution.}
A larger number of trees usually result in a more accurate random forest model, at the cost of longer training and prediction time.
In order to reduce the time cost of performance regression detection, we gradually decreased the number of trees in our random forest model while ensuring the model performance is not significantly impaired.
%Since more trees will give the model better performance and it also makes the code much slower, according to prior research and experience, we reduce this number from 500 to 100. 
At the end, we kept 100 decision trees in our random forest models and spent less than 2 hours to detect performance regressions. % do the same work among two versions. 
Such a lighter model also enables us to detect performance regressions between the current version and multiple history versions (e.g., taking less than 10 hours when comparing the current version with five historical versions). 
%We evaluate the model by comparing the average error percentage with previous model and performing ten-fold cross-validation on three different ES data sets. 
We compared the prediction results of the 100-tree model with the original 500-tree models. We found that the 100-tree model only has a slightly higher median relative error (around 0.1\%) than the previous 500-tree models, which is negligible in practice. % The detailed evaluation result are shown in Table~\ref{tab:modeltreeevaluation}.

\noindent\textbf{Lessons learned.}
%High model performance does not always mean good in industry and performance detection time cost is also an important factor that need to be considered when transfer research to practice. 
%Sometimes we need to make a trade-off between the model performance and  model raining time cost.
In addition to the model performance, the time cost of training and applying a black-box model is also a main concern in the practice of performance regression detection in the field. % that uses black-box models.
Seeking an appropriate trade-off between the model performance and the time cost is essential for the successful adoption of performance regression detection approaches in the field.


\stepcounter{ChallengeCount}
\subsection*{C\arabic{ChallengeCount}: Early detection of performance regressions}
\noindent\textbf{Challenge.}
In an in-house performance testing process, performance engineers usually wait until all the tests finish before they analyze the testing results.
However, in a field environment, any performance regressions can directly impact users' experience.
We cannot usually wait for a long time to gather plenty of data before performing performance analysis.
If our approach cannot detect performance regressions in a timely manner, the performance regressions would already have brought non-negligible negative impact to users.
This is a unique challenge facing the detection of performance regressions in the field.

\noindent\textbf{Solution.}
In order to detect field performance regressions in a timely manner, we continuously apply our approach to detect performance regressions using the currently available data.
For example, after a new version of the system has been up and running for one hour, we use only the one-hour data as our new workloads to determine the existence of performance regressions in the new versions.
Using the data generated in a short time period also allows the analysis part of our approach (i.e., model training and predictions) to be processed faster.
As discussed in RQ2, our approach can effectively detect performance regressions when the system has run for a very short time (e.g., down to 15 minutes for Apache James).
In other words, our approach can detect early performance regressions in the field.

\noindent\textbf{Lessons Learned.}
Different from performance regression detection in an in-house testing environment, performance regressions in the field need to be detected in a timely manner, in order to avoid notable performance impact to the users.
Continuously detecting performance regressions using the currently available data can help detect early performance regressions in the field.
