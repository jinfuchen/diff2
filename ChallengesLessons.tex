\section{Challenges and Lessons Learned} \label{sec:challenges}
In this section, we discuss our faced challenges and learned lessons during applying our approach to the production environment of an industrial setting where a large number of customers worldwide access the system on a daily basis.

\newcounter{ChallengeCount}
\setcounter{ChallengeCount}{0}

\stepcounter{ChallengeCount}
\subsection*{C\arabic{ChallengeCount}: Determining the sampling frequency of performance metrics}
\noindent\textbf{Challenge.}
Our approach uses both logs and performance metrics as the input data to our black-box models. We use the logs that are automatically generated by the web servers, such as the Jetty, Tomcat, and IIS (Internet Information Services) web servers. 
The performance metrics (e.g., CPU, I/O) of the systems are collected using tools (e.g., \emph{Pidstat}).
A higher sampling frequency of the performance metrics can capture the system performance more accurately. However, a higher sampling frequency would also introduce more performance overhead. Since we need to apply our approach to the production environment, it is necessary to produce as low performance overhead as possible.

\noindent\textbf{Solution.}
At a first attempt, we intuitively chose 10 seconds as the sampling interval of the performance metrics. After we deployed our approach in production, we found that there is 0.5\%-0.8\% CPU overhead each time when our approach is collecting the performance metrics, and the overhead happens six times in 1 minute. Such a monitoring overhead cannot be ignored, especially when the system is serving heavy workloads. After working closely with the IT staffs from our industrial partner, we gained a deeper understanding of how the sampling frequency of performance metrics impact the monitoring overhead. Finally, we agreed that collecting the performance metrics for every 30 seconds would be a good balance between reducing the monitoring overhead and ensuring accuracy measurement of the system performance.

\noindent\textbf{Lessons learned.}
Monitoring a system usually comes with the monitoring overhead.
A higher monitoring frequency can provide a better monitoring accuracy at the cost of a larger monitoring overhead, which is usually undesirable when the system is serving large workloads.
Finding a good balance between the monitoring accuracy and the overhead is crucial for successful adoption of similar approaches in practice.


\stepcounter{ChallengeCount}
\subsection*{C\arabic{ChallengeCount}: Reducing the time cost of performance regression detection}
\noindent\textbf{Challenge.}
We choose random forest as our final black-box model to detect performance regressions, as random forest achieves the best results for modeling the performance of the studied systems (see RQ1).
A random forest model contains a configurable multitude of decision trees and takes the average output of the individual decision trees as the final output.
At first, we started with the default number of trees (i.e., 500 trees)~\citep{R-RandomForest}.
It took 5-6 hours to detect performance regressions between two releases of the industry system, including training and testing the random forest models and performing statistical tests. 
The industrial practitioners usually have an early need of checking if there is performance regression between the current version and multiple historical versions, which makes our approaches difficult to be adopted in a fast-paced development environment (e.g., an agile environment).

\noindent\textbf{Solution.}
A larger number of trees usually result in a more accurate random forest model, at the cost of longer training and prediction time.
In order to reduce the time cost of performance regression detection, we gradually decreased the number of trees in our random forest model while ensuring the model performance is not significantly impaired.
In the end, we kept 100 decision trees in our random forest models.
It took less than 2 hours to detect performance regressions between two releases of the industry system (i.e., training and testing the random forest models and performing statistical tests). 
Such a lighter model also enables us to detect performance regressions between the current version and multiple history versions (e.g., taking less than 10 hours when comparing the current version with five historical versions). 
We compared the prediction results of the 100-tree model with the original 500-tree models. We found that the 100-tree model only has a slightly higher median relative error (around 0.1\%) than the previous 500-tree models, which is negligible in practice.

\noindent\textbf{Lessons learned.}
In addition to the model performance, the time cost of training and applying a black-box model is also a major concern in the practice of performance regression detection in the field.
Seeking an appropriate trade-off between the model performance and the time cost is essential for the successful adoption of performance regression detection approaches in the field.


\stepcounter{ChallengeCount}
\subsection*{C\arabic{ChallengeCount}: Early detection of performance regressions}
\noindent\textbf{Challenge.}
In an in-house performance testing process, performance engineers usually wait until all the tests finish before they analyze the testing results.
However, in a field environment, any performance regressions can directly impact users' experience.
We cannot usually wait for a long time to gather plenty of data before performing performance analysis.
If our approach cannot detect performance regressions in a timely manner, the performance regressions would already have brought non-negligible negative impact to users.
This is a unique challenge facing the detection of performance regressions in the field.

\noindent\textbf{Solution.}
In order to detect field performance regressions in a timely manner, we continuously apply our approach to detect performance regressions using the currently available data.
For example, after a new version of the system has been up and running for one hour, we use only the one-hour data as our new workloads to determine the existence of performance regressions in the new versions.
Using the data generated in a short time period also allows the analysis part of our approach (i.e., model training and predictions) to be processed faster.
As discussed in RQ2, our approach can effectively detect performance regressions when the system has run for a very short time (e.g., down to 15 minutes for Apache James).
In other words, our approach can detect early performance regressions in the field.

\noindent\textbf{Lessons Learned.}
Different from performance regression detection in an in-house testing environment, performance regressions in the field need to be detected in a timely manner, in order to avoid notable performance impact to the users.
Continuously detecting performance regressions using the currently available data can help detect early performance regressions in the field.
