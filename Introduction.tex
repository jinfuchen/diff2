\section{Introduction} \label{sec:intro}

Many large-scale software systems (e.g., Amazon, Google, Facebook) provide services to millions or even billions of users every day.
Performance regressions in such systems usually lead to both reputational and monetary losses. 
For example, a recent incident of performance regressions at Salesforce\footnote{https://www.salesforce.com} affected eight data centers, resulting in a negative impact (i.e., slowdown and failures of the provided services) on the experience of clients across many organizations ~\citep{SaleForcePerfDegradation2019}.
It is reported that 59\% of Fortune 500 companies experience at least 1.6 hours of downtime per week~\citep{SyncsortWhitePaper2018}, while most of the field failures are due to performance issues rather than feature issues~\citep{DBLP:journals/tse/WeyukerV00}.
To avoid such performance-related problems, practitioners perform in-house performance testing to detect performance regressions of large-scale software systems~\citep{DBLP:journals/tse/JiangH15}.

For in-house performance testing, load generators (e.g., JMeter\footnote{https://jmeter.apache.org}) are used to mimic thousands or millions of users (i.e., the \emph{workload}) accessing the system under test (SUT) at the same time.
Performance data (e.g., CPU usage) is recorded during the performance testing for later performance analysis.
To determine whether a performance regression exists, the performance data of the current software version is compared to the performance data from running an earlier version on the same workload, using various analysis techniques~\citep{DBLP:conf/icst/GaoJBL16} (e.g., control charts~\citep{DBLP:conf/wosp/NguyenAJHNF12}).
However, such in-house performance testing suffers from two important challenges.
First, the real workloads in the field are constantly changing and difficult to predict~\citep{DBLP:conf/wosp/SyerJNHNF14}, making it unrealistic to test such dynamic workloads in a predefined manner.
Second, performance testing is usually very expensive as it requires extensive resources (e.g., computing power) and lasts for a long time (e.g., hours or even weeks).
For software systems that follow fast release cycles (e.g., Facebook is released every few hours), performance testing becomes particularly difficult.


In this work, we aim at directly detecting performance regressions in the field operations of large-scale software systems.
Such an automated detection can complement or even replace typical in-house performance testing when testing resources are limited (e.g., in an agile environment) and/or filed workloads are challenging to be reproduced in testing.
In particular, we leverage sparsely-sampled performance metrics and readily-available execution logs from the field to detect performance regressions, which only adds negligible performance overhead to SUT.
Inspired by prior research~\citep{Yao:2018:LSL:3184407.3184416,DBLP:conf/issre/FarshchiSWG15}, we use machine learning techniques (i.e., black-box performance models) to capture the relationship between the runtime activities that are recorded in the readily-available logs of a software system and its performance metrics under such activities. 
We use the black-box model that describes the current version of a software system and the black-box model that describes an earlier version of the same software system, to determine the existence of performance regressions between these two versions.

We conducted an empirical study on two open source systems (i.e., OpenMRS and Apache James) and one large-scale industrial system. 
In particular, our study aims to answer two research questions (RQs):

\begin{description}
\item[\textbf{RQ1:}] \textit{How well can we model system performance under varying workloads?} \\ 
In order to capture the performance of a system under varying workloads, we built six machine learning models (including three traditional models and three deep neural networks).
We found that simple traditional models can effectively capture the relationship between the performance of a system and its dynamic activities that are recorded in logs. 
In addition, such models can equivalently capture the performance of a system under new workloads that are unseen when building the models.

\item[\textbf{RQ2:}] \textit{Can our approaches detect performance regressions under varying workloads?} \\ 
Our black-box model-based approaches can effectively detect both the real performance regressions and injected ones under varying workloads (i.e., when the workloads are unseen when training the black-box models).
Besides, our approaches can detect performance regressions with the data from a short period of operations, enabling early detection of performance regressions in the field.

\end{description}

Our work makes the following key contributions:
\begin{itemize} 
    \item Our study demonstrates the effectiveness of using black-box machine learning models to model the performance of a system when the workloads are evolving (i.e., the model trained on a previous workload can capture the system performance under new unseen workloads). %against its dynamic activities in field operations.
    %\item Our study demonstrates the effectiveness of leveraging operational data (e.g., logs and performance metrics) from the field and black-box machine learning models to detect performance regressions in the field.
    \item Our experiments on one large-scale industry system and two open source systems show that we can use black-box machine learning models to detect performance regressions in the field where workloads are constantly evolving (i.e., two releases of the same system are never running the same workloads).
    \item Our approaches can complement or even replace traditional in-house performance testing when testing resources are limited.
    \item We shared the challenges and the lessons that we learned from the successful adoption of our approaches in the industry, which can provide insights for researchers and practitioners who are interested in detecting performance regressions in the field.
\end{itemize}

The remainder of the paper is organized as follows. 
Section~\ref{sec:background} introduces the background of black-box performance modeling.
Section~\ref{sec:approach} outlines our approaches for detecting performance regressions in the field.
Section~\ref{sec:casestudysetup} and Section~\ref{sec:casestudyresults} present the setup and the results of our case study on three subject systems, respectively.
The challenges and the lessons that we learned from the successful adoption of our approaches are discussed in Section~\ref{sec:challenges}.
Section~\ref{sec:threats} and Section~\ref{sec:relatedwork} discuss the threats to the validity of our findings and the related work, respectively.
Finally, Section~\ref{sec:conclusions} concludes the paper.

