\section{Threats to Validity} \label{sec:threats}

This section discusses the threats to the validity of our study.

\noindent \textbf{External validity.}
Our study is performed on two open source systems (i.e., OpenMRS and Apache James) and one industry system (i.e., System X) that are from different domains (e.g., health care system and mail server). 
All our studied systems are mature systems with years of history and have been studied in prior performance engineering research. Nevertheless, more case studies on other software systems in other domains can benefit the evaluation of our approach.
All our studied systems are developed in either Java or .Net. Our approach may not be directly applicable to systems developed in other programming languages, especially dynamic languages (e.g., JavaScript and Python). Future work may investigate approaches to minimize the uncertainty in the performance characterization of systems developed in dynamic languages.

\noindent \textbf{Internal validity.}
Our approach builds machine learning models to capture the relationship between the runtime activities that are recorded in logs and the measured performance of a system. 
However, there might be some runtime activities that are not recorded in logs and that also impact system performance.
In our approach, we use logs that capture the high-level workloads of the system. 
Our experiments on our studied systems demonstrated that such logs can predict system performance with high accuracy.
Nevertheless, the correlation between the runtime activities recorded in the logs and the measured system performance does not necessarily suggest a causal relationship between them.


Our approach relies on non-parametric statistical analysis (i.e., Mann-Whitney U test and Cliff's delta) to compare the black-box behaviors of two software versions to detect performance regressions. 
Our assumption is that statistically different behaviors between two software versions would suggest performance regressions.
In practice, however, determining whether there is a performance regression usually depends on the subjective judgment of the performance analysts.
Therefore, our approach enables performance analysts to adjust the threshold of the statistics (e.g., the effect size) to detect performance regressions in their specific scenarios.

\noindent \textbf{Construct validity.}
In this work, we use the CPU usage as our performance metric to detect performance regressions.
There exist other performance metrics, such as memory utilization and response time that can be considered to detect performance regressions. 
Considering other performance metrics would benefit our approach.
However, monitoring more performance metrics would introduce more performance overhead to the monitored system.
In the case of our industrial system, CPU usage is the main concern in performance regression detection.
Besides, the three studied approaches are not limited to the performance metric of CPU usage. Practitioners can leverage our approach to consider other performance metrics that are appropriate in their context.