\section{Case Study Setup} \label{sec:casestudysetup}
To study the effectiveness of our approaches for detecting performance regressions under varying workloads, we perform case studies on two open source systems and one large-scale industrial system\footnote{Our experimental setup, workloads and results are shared online \url{https://github.com/senseconcordia/EMSE2020Data} as a replication package.}. In this section, we first present the subject systems. Then, we present the workloads applied to the systems, the experimental environment and performance issues. Finally, we present the choices of machine learning models that are used in our approaches. 

\subsection{Subject systems}
We evaluate our approaches with two open-source systems, namely OpenMRS and Apache James, as well as one industrial system (System X). Apache James is a Java-based mail server. OpenMRS is an open-source health care system that supports customized medical records and it is wildly used in developing countries. System X is a commercial software system that provides government-regulation related reporting services. The service is wildly used as the market leader of the domain. Due to a Non-Disclosure Agreement (NDA), we cannot reveal additional details about the system. We do note that System X has over ten years of history with more than two million lines of code that are based on Microsoft .Net. All our subject systems cover different domains and are studied in prior research~\citep{Yao:2018:LSL:3184407.3184416, DBLP:conf/icst/GaoJBL16}. The details of each subject system are shown in Table~\ref{tab:subjects}. 

All three subject systems are CPU-intensive. Besides, CPU is usually the main contributor to server cost~\citep{Greenberg:2008:CCR:1496091.1496103}. Performance regression in terms of CPU would result in the need for more CPU resources to provide the same quality of service, thereby significantly increasing the cost of system operations. 
For example, for our industrial system (System X), CPU usage is the main concern in performance regression detection.
Therefore, in this study, we use CPU as the performance metric for detecting performance regressions.


\begin{table}[tbh]
  \centering
  \caption{Overview of our subject systems}
    \begin{tabular}{c|c|c|r}
    \hline
    Subjects & Versions & Domains & SLOC (K) \\
    \hline
    OpenMRS & 2.1.4 & Medical & 67 \\
    
    Apache James & 2.3.2, 3.0M1, 3.0M2 & Mail Server & 37 \\
    
    System X & 10 releases in 2019 & Commercial & \textgreater 2,000 \\
    \hline
    \end{tabular}%
  \label{tab:subjects}%
\end{table}%


\subsection{Subject workload design}
\subsubsection{OpenMRS}
OpemMRS provides a web-based interface and RESTFul services. We used the default OpenMRS demo database in our performance tests. The demo database contains data for over 5K patients and 476K observations. OpenMRS contains four typical scenarios: adding, deleting, searching, and editing operations. We designed five different performance tests that are composed of eight various test actions, e.g., searches of patients, concepts, encounters, and observations, and creation, deletion and edit of patient information. Those five performance tests are distinguished by the ratio of each action. In order to simulate a more realistic workload in the field, we added random controllers and random order controllers in JMeter to vary the workload. Moreover, we also simulated the variety of the number of users and activities in the field by setting random gaps between the repetitions of each userâ€™s activities, randomizing the order of the user activities and for different workloads and different time, setting a different number of maximum concurrent users. We made sure our workload does not reach saturation. The details of the workload, such as the CPU usage values, are shared in our replication package. The original version, i.e., v0 of OpenMRS does not have any injected or known performance regressions. We separately injected four performance regressions (heavier DB request, additional I/O, constant delay and additional calculation) into v0. These four versions are called v1, v2, v3, and v4, respectively. The details of the injected performance regressions of OpenMRS are shown in Table~\ref{tab:workloaddeisign}.

We deployed the OpenMRS on two machines, each with Intel Core i5-2400 CPU (3.10GHz), 8 GB memory, 512GB SATA hard drive. One machine is deployed as the application server and another machine is deployed as MySQL database server. We used the RESTFul API from OpenMRS and ran JMeter on five extra machines with the same specification to simulate users on the client side with a five-hour workload. Each machine hosts one JMeter instance with one type of workload. We used \emph{Pidstat}~\citep{pidstat} to monitor CPU usage as the performance metrics of this study.
To minimize the noise from the system warm-up and cool-down periods, we filtered out the data from the first and last half an hour of running each workload. Thus, we only kept four hours of data from each performance test.

\subsubsection{Apache James}
Apache James is an open source enterprise mail server. It contains two main scenarios: sending and retrieving mails. Those two actions can be further divided into many smaller scenarios, e.g., sending mails with or without attachments and retrieving entire mail or only the header of the mail. In total, we built eight scenarios for the performance test. Similar to OpenMRS, we also created five different workloads that consist of eight actions of different ratio and performance-test Apache James with varying workloads to simulate the real user operation. We used JMeter to create performance tests that exercise Apache James. Different from OpenMRS, in which the performance regressions are manually injected, we performance-tested on three different versions of systems with known real-world performance issues for Apache James. Apart from the last stable version 2.3.2, there are multiple Milestone Releases for version 3.0. After checking the release notes on the Apache James website, we picked 3.0M1 and 3.0M2 to use in our study. These three selected versions have many bug fixes and performance improvements~\citep{Apache-James}. The same subjects are studied in prior research on performance testing~\citep{DBLP:conf/icst/GaoJBL16}. Table~\ref{tab:workloaddeisign} summarizes the details of performance improvements of three selected versions. 

We deployed Apache James on a server machine with an Intel Core i7-8700K CPU (3.70GHz), 16 GB memory on a 3TB SATA hard drive. We ran JMeter on five extra machines with Intel Core i5-2400 CPU (3.10GHz), 8 GB memory and 320GB SATA hard drive to generate multiple five-hour workloads. Similar to OpenMRS, we use \emph{Pidstat}~\citep{pidstat} to monitor CPU usage as the performance metrics of this study. We filtered out the data from the first and last half an hour of running each workload to minimize the system noises. 

\subsubsection{System X}
System X is deployed in production and is used by real users worldwide. We retrieved the execution logs and the corresponding CPU usage on a daily basis. System X is deployed in an internal production environment. Due to the NDA, we cannot reveal additional details about the hardware environment and the usage scenarios of System X.

\begin{table}[tbh]
  \centering
  \caption{Performance regressions in the studied open source systems}
    \begin{tabular}{c|c|p{3.3cm}}
    \hline
    System & Versions & \multicolumn{1}{c}{Performance regressions} \\
    \hline
          & v0    & \multicolumn{1}{c}{Original version} \\
\cline{2-3}          & v1    & \multicolumn{1}{c}{Injected heavier DB request} \\
\cline{2-3}    OpenMRS & v2    & \multicolumn{1}{c}{Added additional I/O access} \\
\cline{2-3}          & v3    & \multicolumn{1}{c}{Created a constant delay} \\
\cline{2-3}          & v4    & \multicolumn{1}{c}{Injected additional calculation} \\
    \hline
          & 2.3.2 & \multicolumn{1}{c}{Stable release version} \\
          \cline{2-3}
          Apache James & 3.0M1 & \multicolumn{1}{c}{Improved ActiveMQ spool efficiency~\citep{Apache-James}} \\
          \cline{2-3}
          & 3.0M2& \multicolumn{1}{c}{Improved large attachment handling efficiency~\citep{Apache-James}} \\
    \hline
    \end{tabular}%
  \label{tab:workloaddeisign}%
\end{table}%



\subsection{Subject models}
Our approach presented in Section~\ref{sec:approach} is not designed strictly to any particular type of machine learning models. In fact, practitioners may choose their preferred models. In this study, we study the use of six different types of models, namely linear regression, random forest, XGBoost, convolutional neural network (CNN), recurrent neural network (RNN) and long short-term memory (LSTM). In particular, for linear regression, random forest, XGBoost, and CNN, the input of the models are vectors whose values are the number of appearances of each log event in a time period. For RNN and LSTM, we sort the logs in each time period by their corresponding time stamp to create a sequence as the input of the neural networks. Our XGBoost model is fine-tuned using the GridSearchCV~\citep{girdsearcv} method. Our CNN, RNN and LSTM have three, four and four layers, respectively, and they are trained with five, five and ten epochs, respectively. The number of layers and epochs are manually fine-tuned to avoid overfitting.
 
